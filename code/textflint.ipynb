{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mambaforge/envs/trainbert/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-04-23 21:02:48.232430: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-23 21:02:50.111305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-23 21:02:50.112194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-23 21:02:50.112374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import textflint\n",
    "from predict_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test_private_expanded.csv')\n",
    "test2 = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test_public_expanded.csv')\n",
    "test = pd.concat([test1, test2])\n",
    "with multiprocessing.Pool(processes=8) as pool:\n",
    "     text_list = pool.map(normalize, test.comment_text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = (test[\"toxicity\"] >= 0.5).astype(int).astype(str).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = [{\"x\": a, \"y\": b} for a, b in zip(text_list, test_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# with open(\"models/test_list.json\", \"w+\") as f:\n",
    "#     json.dump(json_data, f)\n",
    "with open(\"models/test_list.json\", \"r\") as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': '[ Integrity means that you pay your debts.] Does this apply to President Trump too?',\n",
       " 'y': '0'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textflint.input.component.sample.sa_sample import SASample\n",
    "from textflint.input.dataset import Dataset\n",
    "from textflint.generation.transformation.UT.keyboard import Keyboard\n",
    "from textflint.generation.transformation.UT.spelling_error import SpellingError\n",
    "from textflint.generation.transformation.UT.swap_syn_wordnet import SwapSynWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "  0%|          | 0/194640 [00:00<?, ?it/s]\u001b[34;1mTextFlint\u001b[0m: Downloading http://textflint.oss-cn-beijing.aliyuncs.com/download/SPACY_MODEL/model.zip.\n",
      "100%|██████████| 764M/764M [07:10<00:00, 1.77MB/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: Unzipping file /home/ubuntu/.cache/textflint/tmpwqo9ulfp to /home/ubuntu/.cache/textflint/SPACY_MODEL.\n",
      "\u001b[34;1mTextFlint\u001b[0m: Successfully saved SPACY_MODEL/model.zip to cache.\n",
      "/home/ubuntu/mambaforge/envs/trainbert/lib/python3.7/site-packages/spacy/util.py:887: UserWarning: [W095] Model 'en_core_web_lg' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.5.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "100%|██████████| 194640/194640 [08:57<00:00, 362.18it/s] \n",
      "\u001b[34;1mTextFlint\u001b[0m: 194640 in total, 194640 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3336/1437535313.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtf_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtransform_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf1_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtransform_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf2_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtransform_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf3_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3336/1437535313.py\u001b[0m in \u001b[0;36mtransform_dataset\u001b[0;34m(tf, ds, tf_ds)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransform_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtf_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtransform_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf1_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sa_ds = Dataset(\"SA\")\n",
    "sa_ds.load(json_data)\n",
    "# sa_ds.load_json(\"models/test_list.json\")\n",
    "tf1_ds = Dataset(\"SA\")\n",
    "tf2_ds = Dataset(\"SA\")\n",
    "tf3_ds = Dataset(\"SA\")\n",
    "\n",
    "tf1 = Keyboard()\n",
    "tf2 = SpellingError()\n",
    "tf3 = SwapSynWordNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 176863/194640 [05:41<00:34, 518.47it/s]\n",
      "100%|██████████| 194640/194640 [03:36<00:00, 900.11it/s] \n",
      "100%|██████████| 194640/194640 [29:12<00:00, 111.08it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def transform_dataset(tf, ds, tf_ds):\n",
    "    for s in tqdm(ds):\n",
    "        transformed = tf.transform(s)\n",
    "        if len(transformed) > 0:\n",
    "            tf_ds.append(transformed[0])\n",
    "\n",
    "transform_dataset(tf1, sa_ds, tf1_ds)\n",
    "transform_dataset(tf2, sa_ds, tf2_ds)\n",
    "transform_dataset(tf3, sa_ds, tf3_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mTextFlint\u001b[0m: Save samples to models/test_tf1.csv!\n"
     ]
    }
   ],
   "source": [
    "tf1_ds.save_csv(\"models/test_tf1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mTextFlint\u001b[0m: Save samples to models/test_tf2.csv!\n",
      "\u001b[34;1mTextFlint\u001b[0m: Save samples to models/test_tf3.csv!\n"
     ]
    }
   ],
   "source": [
    "tf2_ds.save_csv(\"models/test_tf2.csv\")\n",
    "tf3_ds.save_csv(\"models/test_tf3.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
